{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60b2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8de53b",
   "metadata": {},
   "source": [
    "Pre-praparing umsatz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f5afc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum\n",
      "0  2013-12-25\n",
      "1  2013-12-26\n",
      "2  2014-01-01\n",
      "3  2014-04-18\n",
      "4  2014-05-01\n",
      "5  2014-05-03\n",
      "6  2014-05-04\n",
      "7  2014-08-17\n",
      "8  2014-12-25\n",
      "9  2014-12-26\n",
      "10 2015-01-01\n",
      "11 2015-04-03\n",
      "12 2015-05-01\n",
      "13 2015-12-25\n",
      "14 2015-12-26\n",
      "15 2016-01-01\n",
      "16 2016-03-24\n",
      "17 2016-03-25\n",
      "18 2016-07-07\n",
      "19 2016-07-18\n",
      "20 2016-08-14\n",
      "21 2016-08-15\n",
      "22 2016-12-17\n",
      "23 2016-12-25\n",
      "24 2016-12-26\n",
      "25 2017-01-01\n",
      "26 2017-04-14\n",
      "27 2017-05-01\n",
      "28 2017-07-04\n",
      "29 2017-10-23\n",
      "30 2017-10-31\n",
      "31 2017-12-25\n",
      "32 2017-12-26\n",
      "33 2018-01-01\n",
      "34 2018-03-30\n",
      "35 2018-05-01\n",
      "36 2018-05-21\n"
     ]
    }
   ],
   "source": [
    "dates_umsatz_null = pd.to_datetime([\n",
    "    \"2013-12-25\", \"2013-12-26\", \"2014-01-01\",\n",
    "    \"2014-04-18\", \"2014-05-01\", \"2014-05-03\",\n",
    "    \"2014-05-04\", \"2014-08-17\", \"2014-12-25\",\n",
    "    \"2014-12-26\", \"2015-01-01\", \"2015-04-03\",\n",
    "    \"2015-05-01\", \"2015-12-25\", \"2015-12-26\",\n",
    "    \"2016-01-01\", \"2016-03-24\", \"2016-03-25\",\n",
    "    \"2016-07-07\", \"2016-07-18\", \"2016-08-14\",\n",
    "    \"2016-08-15\", \"2016-12-17\", \"2016-12-25\",\n",
    "    \"2016-12-26\", \"2017-01-01\", \"2017-04-14\",\n",
    "    \"2017-05-01\", \"2017-07-04\", \"2017-10-23\",\n",
    "    \"2017-10-31\", \"2017-12-25\", \"2017-12-26\",\n",
    "    \"2018-01-01\", \"2018-03-30\", \"2018-05-01\",\n",
    "    \"2018-05-21\"\n",
    "])\n",
    "\n",
    "#format dates_umsatz_null as dataframe\n",
    "df_umsatz_null = pd.DataFrame(dates_umsatz_null, columns=['Datum']) \n",
    "\n",
    "print(df_umsatz_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca6dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Datum\n",
      "0   2018-08-01\n",
      "1   2018-08-02\n",
      "2   2018-08-03\n",
      "3   2018-08-04\n",
      "4   2018-08-05\n",
      "..         ...\n",
      "360 2019-07-27\n",
      "361 2019-07-28\n",
      "362 2019-07-29\n",
      "363 2019-07-30\n",
      "364 2019-07-31\n",
      "\n",
      "[365 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#create new dataframe with all dates from 2018-08-01 to 2019-07-31 and dates_umsatz_null\n",
    "all_dates = pd.date_range(start='2018-08-01', end='2019-07-31')\n",
    "all_dates_df = pd.DataFrame(all_dates, columns=['Datum'])\n",
    "\n",
    "print(all_dates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fc7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Datum\n",
      "0   2013-12-25\n",
      "1   2013-12-26\n",
      "2   2014-01-01\n",
      "3   2014-04-18\n",
      "4   2014-05-01\n",
      "..         ...\n",
      "397 2019-07-27\n",
      "398 2019-07-28\n",
      "399 2019-07-29\n",
      "400 2019-07-30\n",
      "401 2019-07-31\n",
      "\n",
      "[402 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "missing_dates = pd.merge(all_dates_df, df_umsatz_null, on='Datum', how='outer', indicator=False)\n",
    "print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#check for what month and days (no year) warengruppe 6 is avalable every year\n",
    "mask = df_inputated['Warengruppe'] == 6\n",
    "# ensure the 'Datum' values are datetime-like before using .dt (do not overwrite original column)\n",
    "date_series = pd.to_datetime(df_inputated.loc[mask, 'Datum'], errors='coerce')\n",
    "warengruppe6_dates = date_series.dt.strftime('%m-%d').dropna().unique()\n",
    "print(\"Warengruppe 6 is available on the following month-day combinations:\")\n",
    "print(warengruppe6_dates)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2505dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "warengruppe6_dates = [\n",
    "    '10-24', '10-25', '10-26', '10-27', '10-29', '10-30', '10-31',\n",
    "    '11-01', '11-02', '11-03', '11-04', '11-05', '11-06', '11-07',\n",
    "    '11-08', '11-09', '11-10', '11-11', '11-12', '11-13', '11-14',\n",
    "    '11-15', '11-16', '11-17', '11-18', '11-19', '11-20', '11-21',\n",
    "    '11-22', '11-23', '11-24', '11-25', '11-26', '11-27', '11-28',\n",
    "    '11-29', '11-30', '12-01', '12-02', '12-03', '12-04', '12-05',\n",
    "    '12-06', '12-07', '12-08', '12-09', '12-10', '12-11', '12-12',\n",
    "    '12-13', '12-14', '12-15', '12-16', '12-17', '12-18', '12-19',\n",
    "    '12-20', '12-21', '12-22', '12-23', '12-24', '12-27', '12-29',\n",
    "    '12-30', '01-02', '10-28', '12-28', '01-03', '01-04', '01-05',\n",
    "    '01-06', '12-31'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c26ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(df_daten, warengruppe6_dates, dates_umsatz_null):\n",
    "    df = df_daten.copy()\n",
    "\n",
    "    # 1) Ensure Datum is datetime\n",
    "    df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "    # 2) Create base with all unique dates\n",
    "    base_dates = df['Datum'].drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    base = pd.DataFrame({'Datum': base_dates})\n",
    "\n",
    "    # 3) Determine if each date should have 5 or 6 groups (compare MM-DD)\n",
    "    wg6 = set(warengruppe6_dates)\n",
    "    base['mmdd'] = base['Datum'].dt.strftime('%m-%d')\n",
    "    base['n_groups'] = np.where(base['mmdd'].isin(wg6), 6, 5)\n",
    "\n",
    "    # 4) Expand each date into 1..5 or 1..6 Warengruppe\n",
    "    base['Warengruppe'] = base['n_groups'].apply(lambda n: list(range(1, n + 1)))\n",
    "    base = base.explode('Warengruppe').drop(columns=['mmdd', 'n_groups'])\n",
    "    base['Warengruppe'] = base['Warengruppe'].astype(int)\n",
    "\n",
    "    # 5) Bring original Umsatz if it exists in df_daten\n",
    "    if 'Warengruppe' in df.columns:\n",
    "        df_model = base.merge(\n",
    "            df[['Datum', 'Warengruppe', 'Umsatz']],\n",
    "            on=['Datum', 'Warengruppe'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        # If df_daten has no Warengruppe column, create empty Umsatz column\n",
    "        df_model = base.copy()\n",
    "        df_model['Umsatz'] = np.nan\n",
    "\n",
    "    # 6) Set Umsatz = 0 for dates in dates_umsatz_null\n",
    "    if dates_umsatz_null is not None and len(dates_umsatz_null) > 0:\n",
    "        dates_null = pd.to_datetime(dates_umsatz_null)\n",
    "        mask_null = df_model['Datum'].dt.normalize().isin(dates_null.normalize())\n",
    "        df_model.loc[mask_null, 'Umsatz'] = 0\n",
    "\n",
    "    # Optional: fill remaining NaNs with 0\n",
    "    # df_model['Umsatz'] = df_model['Umsatz'].fillna(0)\n",
    "\n",
    "    # 7) Create id = YYMMDD + Warengruppe\n",
    "    df_model['id'] = df_model['Datum'].dt.strftime('%y%m%d') + df_model['Warengruppe'].astype(str)\n",
    "\n",
    "    # Order columns nicely\n",
    "    cols = ['id', 'Datum', 'Warengruppe', 'Umsatz']\n",
    "    other_cols = [c for c in df_model.columns if c not in cols]\n",
    "    df_model = df_model[cols + other_cols]\n",
    "\n",
    "    return df_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92223b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = prep_dataset(missing_dates, warengruppe6_dates, dates_umsatz_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78ee7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id       Datum  Warengruppe      Umsatz\n",
      "0  1307011  2013-07-01            1  148.828353\n",
      "1  1307021  2013-07-02            1  159.793757\n",
      "2  1307031  2013-07-03            1  111.885594\n",
      "3  1307041  2013-07-04            1  168.864941\n",
      "4  1307051  2013-07-05            1  171.280754\n"
     ]
    }
   ],
   "source": [
    "df_daten = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Raw/umsatzdaten_gekuerzt.csv')\n",
    "print(df_daten.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c85124ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure Datum column in df_daten is datetime to match df_missing\n",
    "df_daten['Datum'] = pd.to_datetime(df_daten['Datum'])\n",
    "\n",
    "# Ensure 'id' dtypes match before merging (avoid int vs object error).\n",
    "# Convert both to string to be safe (ids like '1907311' preserve formatting).\n",
    "df_daten['id'] = df_daten['id'].astype(str)\n",
    "df_missing['id'] = df_missing['id'].astype(str)\n",
    "\n",
    "# merge df_daten with df_missing to include dates with Umsatz = 0\n",
    "df_umsatz_merged = df_daten.merge(df_missing, on=[\"Datum\", \"id\", \"Warengruppe\", \"Umsatz\"], how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a260d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id      Datum  Warengruppe      Umsatz\n",
      "0      1307011 2013-07-01            1  148.828353\n",
      "1      1307012 2013-07-01            2  535.856285\n",
      "2      1307013 2013-07-01            3  201.198426\n",
      "3      1307014 2013-07-01            4   65.890169\n",
      "4      1307015 2013-07-01            5  317.475875\n",
      "...        ...        ...          ...         ...\n",
      "11413  1907311 2019-07-31            1         NaN\n",
      "11414  1907312 2019-07-31            2         NaN\n",
      "11415  1907313 2019-07-31            3         NaN\n",
      "11416  1907314 2019-07-31            4         NaN\n",
      "11417  1907315 2019-07-31            5         NaN\n",
      "\n",
      "[11418 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_umsatz_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9601299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                0\n",
      "Datum             0\n",
      "Warengruppe       0\n",
      "Umsatz         1897\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#missing values in each column\n",
    "print(df_umsatz_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbf4588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "df_umsatz_merged.to_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/full_umsatzdaten.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
