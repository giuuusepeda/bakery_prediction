{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde83e1f",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbae61",
   "metadata": {},
   "source": [
    "Steps\n",
    "1. Load cvs files\n",
    "2. Merge files\n",
    "3. correct data types\n",
    "4. inputate NA\n",
    "5. Feature engeneering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c4c29e",
   "metadata": {},
   "source": [
    "to do \n",
    "needed another type of join bc the other data will be used for test data \n",
    "apply one hot encoding - to weekday(maybe change to 01 weekend?), new weathercode....\n",
    "moving average for sales, temperature(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008437c",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
=======
   "execution_count": 7,
   "id": "23f63be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad0f62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.3.1)\n",
      "Collecting datetime\n",
      "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Downloading zope_interface-8.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Downloading zope_interface-8.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
      "Installing collected packages: zope.interface, datetime\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [datetime]rface]\n",
      "\u001b[1A\u001b[2KSuccessfully installed datetime-5.5 zope.interface-8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "f2119937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d237e1",
   "metadata": {},
   "source": [
    "1. Load cvs files"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
=======
<<<<<<< Updated upstream
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "6bb431f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\giuli\\\\Documents\\\\Open_Campus\\\\bakery_prediction\\\\0_DataPreparation\\\\Raw\\\\kiwo.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load a CSV files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_kiwo = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mgiuli\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDocuments\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mOpen_Campus\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbakery_prediction\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m0_DataPreparation\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mRaw\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mkiwo.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_daten = pd.read_csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgiuli\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOpen_Campus\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbakery_prediction\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m0_DataPreparation\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRaw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mumsatzdaten_gekuerzt.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m df_wetter = pd.read_csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgiuli\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOpen_Campus\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mbakery_prediction\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m0_DataPreparation\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRaw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mwetter.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\giuli\\\\Documents\\\\Open_Campus\\\\bakery_prediction\\\\0_DataPreparation\\\\Raw\\\\kiwo.csv'"
     ]
    }
   ],
   "source": [
    "# Load a CSV files\n",
    "df_kiwo = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Raw/kiwo.csv')\n",
    "df_daten = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Raw/umsatzdaten_gekuerzt.csv')\n",
<<<<<<< HEAD
    "df_wetter = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Raw/wetter.csv')"
=======
    "df_wetter = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Raw/wetter.csv')\n"
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
=======
   "execution_count": 4,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "c865fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts to date and sets index\n",
    "df_kiwo['Datum'] = pd.to_datetime(df_kiwo['Datum'], format='%Y-%m-%d')\n",
    "\n",
    "df_daten['Datum'] = pd.to_datetime(df_daten['Datum'], format='%Y-%m-%d')\n",
    "\n",
    "df_wetter['Datum'] = pd.to_datetime(df_wetter['Datum'], format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5d02f7",
   "metadata": {},
   "source": [
    "Creates id ang warengruppe for day with Umsatz 0 and for test dates"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "206ab070",
=======
   "execution_count": 5,
   "id": "0d826aac",
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "metadata": {},
   "outputs": [],
   "source": [
    "warengruppe6_dates = pd.to_datetime([\n",
    "    '10-24' '10-25' '10-26' '10-27' '10-29' '10-30' '10-31' '11-01' '11-02' '11-03' '11-04' '11-05' '11-06' '11-07' '11-08' '11-09' '11-10' '11-11' '11-12' '11-13' '11-14' '11-15' '11-16' '11-17' '11-18' '11-19' '11-20' '11-21' '11-22' '11-23' '11-24' '11-25' '11-26' '11-27' '11-28' '11-29' '11-30' '12-01' '12-02' '12-03' '12-04' '12-05' '12-06' '12-07' '12-08' '12-09' '12-10' '12-11' '12-12' '12-13' '12-14' '12-15' '12-16' '12-17''12-18' '12-19' '12-20' '12-21' '12-22' '12-23' '12-24' '12-27' '12-29' '12-30' '01-02' '10-28' '12-28' '01-03' '01-04' '01-05' '01-06' '12-31'])\n",
    "\n",
<<<<<<< HEAD
    "dates_umsatz_null = pd.to_datetime([\n",
    "    \"2013-12-25\", \"2013-12-26\", \"2014-01-01\",\n",
    "    \"2014-04-18\", \"2014-05-01\", \"2014-05-03\",\n",
    "    \"2014-05-04\", \"2014-08-17\", \"2014-12-25\",\n",
    "    \"2014-12-26\", \"2015-01-01\", \"2015-04-03\",\n",
    "    \"2015-05-01\", \"2015-12-25\", \"2015-12-26\",\n",
    "    \"2016-01-01\", \"2016-03-24\", \"2016-03-25\",\n",
    "    \"2016-07-07\", \"2016-07-18\", \"2016-08-14\",\n",
    "    \"2016-08-15\", \"2016-12-17\", \"2016-12-25\",\n",
    "    \"2016-12-26\", \"2017-01-01\", \"2017-04-14\",\n",
    "    \"2017-05-01\", \"2017-07-04\", \"2017-10-23\",\n",
    "    \"2017-10-31\", \"2017-12-25\", \"2017-12-26\",\n",
    "    \"2018-01-01\", \"2018-03-30\", \"2018-05-01\",\n",
    "    \"2018-05-21\"\n",
    "])\n",
=======
    "print(\"kiwo antes:\", year_counts(df_kiwo['Datum']))\n",
    "print(\"wetter antes:\", year_counts(df_wetter['Datum']))\n",
    "print(\"daten antes:\", year_counts(df_daten['Datum']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05478783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiwo ∩ wetter: 2012    9\n",
      "2013    9\n",
      "2014    9\n",
      "2015    9\n",
      "2016    9\n",
      "2017    9\n",
      "2018    9\n",
      "2019    9\n",
      "Name: count, dtype: int64\n",
      "kiwo ∩ daten : 2014    9\n",
      "2015    9\n",
      "2016    9\n",
      "2017    9\n",
      "2018    9\n",
      "Name: count, dtype: int64\n",
      "wetter ∩ daten: 2013    181\n",
      "2014    357\n",
      "2015    360\n",
      "2016    355\n",
      "2017    355\n",
      "2018    208\n",
      "Name: count, dtype: int64\n",
      "kiwo ∩ wetter ∩ daten por ano: 2014    9\n",
      "2015    9\n",
      "2016    9\n",
      "2017    9\n",
      "2018    9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def overlap_by_year(a, b):\n",
    "    inter = pd.Index(a.dropna().unique()).intersection(pd.Index(b.dropna().unique()))\n",
    "    return pd.Series(inter).dt.year.value_counts().sort_index()\n",
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
    "\n",
    "def prepare_dataset(df_daten, warengruppe6_dates=None, dates_umsatz_null=None):\n",
    "    \"\"\"\n",
    "    Prepara o dataset para modelagem:\n",
    "    - Expande todas as datas em Warengruppe 1..5 ou 1..6 (se estiver em warengruppe6_dates)\n",
    "    - Replica features diárias para cada Warengruppe\n",
    "    - Junta Umsatz original (quando existir)\n",
    "    - Coloca Umsatz = 0 para datas em dates_umsatz_null ou sem venda\n",
    "    - Cria coluna id = YYMMDD + Warengruppe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df_inputated.copy()\n",
    "    \n",
    "    # Garantir tipo datetime\n",
    "    df[\"Datum\"] = pd.to_datetime(df[\"Datum\"])\n",
    "    \n",
    "    # Normalizar listas de datas (podem vir como lista de strings)\n",
    "    if warengruppe6_dates is None:\n",
    "        warengruppe6_dates = pd.to_datetime([])\n",
    "    else:\n",
    "        warengruppe6_dates = pd.to_datetime(warengruppe6_dates)\n",
    "    \n",
    "    if dates_umsatz_null is None:\n",
    "        dates_umsatz_null = pd.to_datetime([])\n",
    "    else:\n",
    "        dates_umsatz_null = pd.to_datetime(dates_umsatz_null)\n",
    "    \n",
    "    # --- 1) Features por data (clima, KielerWoche etc.) ---\n",
    "    # Vamos pegar todas as colunas exceto Warengruppe, Umsatz e id\n",
    "    cols_date_level = [c for c in df.columns if c not in [\"Warengruppe\", \"Umsatz\", \"id\"]]\n",
    "    \n",
    "    # Um registro por data (assumindo que clima, feriado, etc. é igual no dia todo)\n",
    "    date_features = df[cols_date_level].drop_duplicates(subset=[\"Datum\"])\n",
    "    \n",
    "    # Lista de datas únicas\n",
    "    unique_dates = date_features[\"Datum\"].drop_duplicates().sort_values()\n",
    "    \n",
    "    # --- 2) Criar grade completa Datum x Warengruppe ---\n",
    "    rows = []\n",
    "    for date in unique_dates:\n",
    "        if date in warengruppe6_dates:\n",
    "            groups = range(1, 7)   # 1..6\n",
    "        else:\n",
    "            groups = range(1, 6)   # 1..5\n",
    "        \n",
    "        for g in groups:\n",
    "            rows.append({\"Datum\": date, \"Warengruppe\": g})\n",
    "    \n",
    "    full_grid = pd.DataFrame(rows)\n",
    "    \n",
    "    # --- 3) Juntar features diárias na grade ---\n",
    "    df_expanded = full_grid.merge(date_features, on=\"Datum\", how=\"left\")\n",
    "    \n",
    "    # --- 4) Juntar Umsatz original (por Datum + Warengruppe) ---\n",
    "    if \"Warengruppe\" in df.columns:\n",
    "        umsatz_source = df[[\"Datum\", \"Warengruppe\", \"Umsatz\"]].copy()\n",
    "        df_expanded = df_expanded.merge(\n",
    "            umsatz_source,\n",
    "            on=[\"Datum\", \"Warengruppe\"],\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_orig\")\n",
    "        )\n",
    "    else:\n",
    "        # se não existir Warengruppe no original, só cria a coluna\n",
    "        df_expanded[\"Umsatz\"] = pd.NA\n",
    "    \n",
    "    # --- 5) Tratar Umsatz = 0 para datas sem venda ---\n",
    "    # Para datas explicitamente sem venda\n",
    "    df_expanded.loc[df_expanded[\"Datum\"].isin(dates_umsatz_null), \"Umsatz\"] = 0\n",
    "    \n",
    "    # Qualquer NaN restante também vira 0\n",
    "    df_expanded[\"Umsatz\"] = df_expanded[\"Umsatz\"].fillna(0)\n",
    "    \n",
    "    # --- 6) Criar coluna id ---\n",
    "    df_expanded[\"id\"] = (\n",
    "        df_expanded[\"Datum\"].dt.strftime(\"%y%m%d\") +\n",
    "        df_expanded[\"Warengruppe\"].astype(int).astype(str)\n",
    "    )\n",
    "    \n",
    "    # --- 7) Ordenar e retornar ---\n",
    "    df_expanded = df_expanded.sort_values([\"Datum\", \"Warengruppe\"]).reset_index(drop=True)\n",
    "    \n",
    "    return df_expanded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd213d",
   "metadata": {},
   "source": [
    "2. Merge files"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
=======
   "execution_count": 7,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "367865f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes on a common column\n",
    "df_kiwowetter = pd.merge(df_daten, df_kiwo, on='Datum', how='outer')\n",
    "\n",
    "df_merged = pd.merge(df_kiwowetter, df_wetter, on='Datum', how='outer')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 8,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "a02de809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id      Datum  Warengruppe  Umsatz  KielerWoche  Bewoelkung  Temperatur  \\\n",
      "0 NaN 2012-01-01          NaN     NaN          NaN         8.0      9.8250   \n",
      "1 NaN 2012-01-02          NaN     NaN          NaN         7.0      7.4375   \n",
      "2 NaN 2012-01-03          NaN     NaN          NaN         8.0      5.5375   \n",
      "3 NaN 2012-01-04          NaN     NaN          NaN         4.0      5.6875   \n",
      "4 NaN 2012-01-05          NaN     NaN          NaN         6.0      5.3000   \n",
      "\n",
      "   Windgeschwindigkeit  Wettercode  \n",
      "0                 14.0        58.0  \n",
      "1                 12.0         NaN  \n",
      "2                 18.0        63.0  \n",
      "3                 19.0        80.0  \n",
      "4                 23.0        80.0  \n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 rows\n",
    "print(df_merged.head())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
   "id": "2d3d4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset based on datum from 01.07.2013 to 31.07.2019.\n",
    "df_merged = df_merged[(df_merged['Datum'] >= '2013-07-01') & (df_merged['Datum'] <= '2019-07-31')]\n",
=======
   "execution_count": 9,
   "id": "9e8a26ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data range in merged dataframe: 2012-01-01 00:00:00 to 2019-08-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#max and min datum \n",
    "print(\"Data range in merged dataframe:\", df_merged['Datum'].min(), \"to\", df_merged['Datum'].max())\n",
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccd65af",
   "metadata": {},
   "source": [
    "3. correct data types"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
=======
   "execution_count": 6,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "f6e071f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "                 id                          Datum  Warengruppe       Umsatz  \\\n",
      "count  9.334000e+03                           9724  9334.000000  9334.000000   \n",
      "mean   1.559311e+06  2016-02-23 14:29:16.314273792     3.088172   206.749044   \n",
      "min    1.307011e+06            2013-07-01 00:00:00     1.000000     7.051201   \n",
      "25%    1.410123e+06            2014-10-29 00:00:00     2.000000    96.897441   \n",
      "50%    1.601102e+06            2016-02-14 12:00:00     3.000000   161.900831   \n",
      "75%    1.704223e+06            2017-06-15 00:00:00     4.000000   280.644663   \n",
      "max    1.807315e+06            2019-07-31 00:00:00     6.000000  1879.461831   \n",
      "std    1.512503e+05                            NaN     1.489002   144.545189   \n",
      "\n",
      "       KielerWoche  Bewoelkung   Temperatur  Windgeschwindigkeit   Wettercode  \n",
      "count        232.0  9653.00000  9708.000000          9708.000000  7337.000000  \n",
      "mean           1.0     4.72796    12.021734            10.992274    36.716369  \n",
      "min            1.0     0.00000    -8.475000             3.000000     0.000000  \n",
      "25%            1.0     3.00000     6.250000             8.000000    10.000000  \n",
      "50%            1.0     6.00000    11.625000            10.000000    28.000000  \n",
      "75%            1.0     7.00000    17.937500            13.000000    61.000000  \n",
      "max            1.0     8.00000    32.671428            35.000000    95.000000  \n",
      "std            0.0     2.64547     7.221516             4.134352    27.117069  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9724 entries, 394 to 10117\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   id                   9334 non-null   float64       \n",
      " 1   Datum                9724 non-null   datetime64[ns]\n",
      " 2   Warengruppe          9334 non-null   float64       \n",
      " 3   Umsatz               9334 non-null   float64       \n",
      " 4   KielerWoche          232 non-null    float64       \n",
      " 5   Bewoelkung           9653 non-null   float64       \n",
      " 6   Temperatur           9708 non-null   float64       \n",
      " 7   Windgeschwindigkeit  9708 non-null   float64       \n",
      " 8   Wettercode           7337 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(8)\n",
      "memory usage: 759.7 KB\n",
      "None\n",
      "id                      390\n",
      "Datum                     0\n",
      "Warengruppe             390\n",
      "Umsatz                  390\n",
      "KielerWoche            9492\n",
      "Bewoelkung               71\n",
      "Temperatur               16\n",
      "Windgeschwindigkeit      16\n",
      "Wettercode             2387\n",
      "dtype: int64\n"
=======
      "                 id  Warengruppe       Umsatz  KielerWoche    Bewoelkung  \\\n",
      "count  9.334000e+03  9334.000000  9334.000000        250.0  10048.000000   \n",
      "mean   1.559311e+06     3.088172   206.749044          1.0      4.748507   \n",
      "std    1.512503e+05     1.489002   144.545189          0.0      2.628285   \n",
      "min    1.307011e+06     1.000000     7.051201          1.0      0.000000   \n",
      "25%    1.410123e+06     2.000000    96.897441          1.0      3.000000   \n",
      "50%    1.601102e+06     3.000000   161.900831          1.0      6.000000   \n",
      "75%    1.704223e+06     4.000000   280.644663          1.0      7.000000   \n",
      "max    1.807315e+06     6.000000  1879.461831          1.0      8.000000   \n",
      "\n",
      "         Temperatur  Windgeschwindigkeit   Wettercode  \n",
      "count  10103.000000         10103.000000  7581.000000  \n",
      "mean      12.014560            11.026527    37.072022  \n",
      "std        7.212466             4.131774    27.207627  \n",
      "min      -10.250000             3.000000     0.000000  \n",
      "25%        6.250000             8.000000    10.000000  \n",
      "50%       11.625000            10.000000    28.000000  \n",
      "75%       17.875000            13.000000    61.000000  \n",
      "max       32.671428            35.000000    95.000000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10119 entries, 0 to 10118\n",
      "Data columns (total 9 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   9334 non-null   float64\n",
      " 1   Datum                10119 non-null  object \n",
      " 2   Warengruppe          9334 non-null   float64\n",
      " 3   Umsatz               9334 non-null   float64\n",
      " 4   KielerWoche          250 non-null    float64\n",
      " 5   Bewoelkung           10048 non-null  float64\n",
      " 6   Temperatur           10103 non-null  float64\n",
      " 7   Windgeschwindigkeit  10103 non-null  float64\n",
      " 8   Wettercode           7581 non-null   float64\n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 711.6+ KB\n",
      "None\n"
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
     ]
    }
   ],
   "source": [
    "#descriptive statistics\n",
    "print(df_merged.describe())\n",
    "\n",
    "#data types and non-null counts\n",
    "print(df_merged.info())\n",
    "\n",
    "#missing values in each column\n",
    "print(df_merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 62,
=======
   "execution_count": 11,
   "id": "8372ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 'Datum' column to datetime\n",
    "df_merged['Datum'] = pd.to_datetime(df_merged['Datum'], errors='coerce').dt.normalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "d7c86191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set warengruppe, bewoelkung, windgeschwindigkeit, wetter code and kieler woche as integer\n",
    "df_merged['Warengruppe'] = df_merged['Warengruppe'].astype('Int64')\n",
<<<<<<< HEAD
    "df_merged['Bewoelkung'] = df_merged['Bewoelkung'].astype('Int64')\n",
=======
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
    "df_merged['Windgeschwindigkeit'] = df_merged['Windgeschwindigkeit'].astype('Int64')\n",
    "df_merged['Wettercode'] = df_merged['Wettercode'].astype('Int64')\n",
    "df_merged['KielerWoche'] = df_merged['KielerWoche'].astype('Int64')"
   ]
  },
  {
<<<<<<< HEAD
=======
   "cell_type": "code",
   "execution_count": 14,
   "id": "df67459b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#check if the key datum + warengruppe is unique\n",
    "print(df_merged.duplicated(subset=['Datum', 'Warengruppe']).sum()) #should be 0"
   ]
  },
  {
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "cell_type": "markdown",
   "id": "e48d6f17",
   "metadata": {},
   "source": [
    "Save to csv// import from csv"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 63,
=======
   "execution_count": 16,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "13c1659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save merged dataframe to a new CSV file\n",
<<<<<<< HEAD
    "df_merged.to_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/merged_data.csv', index=False)"
=======
    "df_merged.to_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/mergedouter_data.csv', index=False)"
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 77,
=======
   "execution_count": 5,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "33a969ed",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df_merged = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/merged_data.csv')"
=======
    "df_merged = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/mergedouter_data.csv')"
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b44f4",
   "metadata": {},
   "source": [
    "4. inputate NA"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 112,
=======
   "execution_count": 17,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "30139aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputated = df_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 130,
=======
   "execution_count": 18,
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "id": "26b40472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "id                       38\n",
      "Datum                     0\n",
      "Warengruppe              38\n",
      "Umsatz                 1798\n",
      "KielerWoche            1760\n",
      "Bewoelkung             1760\n",
      "Temperatur             1760\n",
      "Windgeschwindigkeit    1760\n",
      "Wettercode             4092\n",
=======
      "id                      785\n",
      "Datum                     0\n",
      "Warengruppe             785\n",
      "Umsatz                  785\n",
      "KielerWoche            9869\n",
      "Bewoelkung               71\n",
      "Temperatur               16\n",
      "Windgeschwindigkeit      16\n",
      "Wettercode             2538\n",
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#missing values in each column\n",
    "print(df_inputated.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7f4e3f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN dentro da máscara: 352\n",
      "NaN fora da máscara: 38\n"
     ]
    }
   ],
   "source": [
    "# Quantos NaN em Warengruppe dentro da máscara?\n",
    "mask = (df_merged['Datum'] >= '2018-08-01') & (df_merged['Datum'] <= '2019-07-31')\n",
    "\n",
    "print(\"NaN dentro da máscara:\", df_merged.loc[mask, \"Warengruppe\"].isna().sum())\n",
    "print(\"NaN fora da máscara:\", df_merged.loc[~mask, \"Warengruppe\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db25c7",
   "metadata": {},
   "source": [
    "Categorical Wettercode per week mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669baa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# derive week and month for df_inputated\n",
    "df_inputated['Woche'] = pd.to_datetime(df_inputated['Datum']).dt.isocalendar().week\n",
    "df_inputated['Monat'] = pd.to_datetime(df_inputated['Datum']).dt.month\n",
    "\n",
    "# precompute weekly and monthly modes (ignore NaNs when computing mode)\n",
    "weekly_mode_map = df_inputated.groupby('Woche')['Wettercode'].agg(\n",
    "    lambda s: s.dropna().mode().iloc[0] if not s.dropna().mode().empty else pd.NA\n",
    ").to_dict()\n",
    "\n",
    "monthly_mode_map = df_inputated.groupby('Monat')['Wettercode'].agg(\n",
    "    lambda s: s.dropna().mode().iloc[0] if not s.dropna().mode().empty else pd.NA\n",
    ").to_dict()\n",
    "\n",
    "# fill missing Wettercode: use weekly mode, if not available fallback to monthly mode\n",
    "def fill_wettercode_row(row):\n",
    "    if pd.isna(row['Wettercode']):\n",
    "        w_mode = weekly_mode_map.get(row['Woche'], pd.NA)\n",
    "        if not pd.isna(w_mode):\n",
    "            return w_mode\n",
    "        return monthly_mode_map.get(row['Monat'], pd.NA)\n",
    "    return row['Wettercode']\n",
    "\n",
    "df_inputated['Wettercode'] = df_inputated.apply(fill_wettercode_row, axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67940ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# 1) Compare antes vs depois\n",
    "print(\"ANTES:\", df_merged['KielerWoche'].value_counts(dropna=False))\n",
    "print(\"DEPOIS:\", df_inputated['KielerWoche'].value_counts(dropna=False))\n",
    "\n",
    "# 2) Quais linhas viraram 0?\n",
    "flip = (df_merged['KielerWoche'] == 1) & (df_inputated['KielerWoche'] == 0)\n",
    "print(\"Flips:\", flip.sum())\n",
    "print(df_merged.loc[flip, ['Datum']].assign(KW_before=1).head())\n",
    "\n",
    "# 3) Veja se há múltiplas colunas de KielerWoche após os merges\n",
    "[k for k in df_merged.columns if 'KielerWoche' in k]\n",
    "[k for k in df_inputated.columns if 'KielerWoche' in k]'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25ed30",
   "metadata": {},
   "source": [
    "set not kiwo to 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dc0504ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3163/3859340180.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_inputated['KielerWoche'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#fix kieler woche missing values by filling with 0\n",
    "df_inputated['KielerWoche'].fillna(0, inplace=True)\n",
    "\n",
    "#set 'KielerWoche' as integer (nullable int64 to allow current NaNs)\n",
    "df_inputated['KielerWoche'] = df_inputated['KielerWoche'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fae48",
   "metadata": {},
   "source": [
    "missing Bewoelkung, Temperatur and/or Windgeschwindigkeit\n",
    "Inputated with mean of day after and before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9fe93e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Bewoelkung  Temperatur  Windgeschwindigkeit\n",
      "6371         NaN         NaN                  NaN\n",
      "6372         NaN         NaN                  NaN\n",
      "6373         NaN         NaN                  NaN\n",
      "6374         NaN         NaN                  NaN\n",
      "6375         NaN         NaN                  NaN\n",
      "...          ...         ...                  ...\n",
      "8019         NaN       8.625                  9.0\n",
      "8020         NaN       8.625                  9.0\n",
      "8021         NaN       8.625                  9.0\n",
      "8022         NaN       8.625                  9.0\n",
      "8023         NaN       8.625                  9.0\n",
      "\n",
      "[71 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#what days are missing Bewoelkung, Temperatur and/or     Windgeschwindigkeit\n",
    "missing_weather = df_inputated[df_inputated[['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit']].isnull().any(axis=1)]\n",
    "print(missing_weather[[ 'Bewoelkung', 'Temperatur', 'Windgeschwindigkeit']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e8a21",
   "metadata": {},
   "source": [
    "fill Bewoelkung, Temperatur, Windgeschwindigkeit to mean of day before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a009e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing weather data with the mean of day before and after\n",
    "df_inputated['Bewoelkung'] = df_inputated['Bewoelkung'].interpolate()\n",
    "df_inputated['Temperatur'] = df_inputated['Temperatur'].interpolate()\n",
    "df_inputated['Windgeschwindigkeit'] = df_inputated['Windgeschwindigkeit'].interpolate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8a0c7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set warengruppe, bewoelkung, windgeschwindigkeit, wetter code and kieler woche as integer\n",
    "df_inputated['Bewoelkung'] = df_inputated['Bewoelkung'].astype('int64')\n",
    "df_inputated['Windgeschwindigkeit'] = df_inputated['Windgeschwindigkeit'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32138d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Bewoelkung before imputation: 4.922250316055626\n",
      "Mean Bewoelkung after imputation: 4.868723532970357\n",
      "Mean Temperatur before imputation: 12.174979272449603\n",
      "Mean Temperatur after imputation: 12.020863154668861\n",
      "Mean Windgeschwindigkeit before imputation: 10.992274412855377\n",
      "Mean Windgeschwindigkeit after imputation: 10.99156725627314\n"
     ]
    }
   ],
   "source": [
    "# median values before and after imputation from 2016-12-11 to 2017-11-08\n",
    "df_merged_ = df_merged[(df_merged['Datum'] >= '2016-12-11') & (df_merged['Datum'] <= '2017-11-08')]\n",
    "df_inputated_ = df_inputated[(df_inputated['Datum'] >= '2016-12-11') & (df_inputated['Datum'] <= '2017-11-08')]\n",
    "\n",
    "print(\"Mean Bewoelkung before imputation:\", df_merged_['Bewoelkung'].mean())\n",
    "print(\"Mean Bewoelkung after imputation:\", df_inputated_['Bewoelkung'].mean())\n",
    "print(\"Mean Temperatur before imputation:\", df_merged_['Temperatur'].mean())\n",
    "print(\"Mean Temperatur after imputation:\", df_inputated['Temperatur'].mean())\n",
    "print(\"Mean Windgeschwindigkeit before imputation:\", df_merged['Windgeschwindigkeit'].mean())\n",
    "print(\"Mean Windgeschwindigkeit after imputation:\", df_inputated['Windgeschwindigkeit'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d6d0a",
   "metadata": {},
   "source": [
    "Discrepancies of Bewoelkung, Temperatur and/or     Windgeschwindigkeit for the same day across WarenGruppe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a87d7c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrepancies in weather data for the same date:\n",
      "           Datum  Bewoelkung  Temperatur  Windgeschwindigkeit\n",
      "1259  2016-12-11           1           6                    6\n",
      "1556  2017-10-04           2           5                    1\n",
      "1557  2017-10-05           2           5                    1\n",
      "1587  2017-11-04           2           1                    1\n",
      "1589  2017-11-06           2           1                    1\n"
     ]
    }
   ],
   "source": [
    "# check if there are discrepancies for the same date for the columns 'Bewoelkung', 'Temperatur', 'Windgeschwindigkeit'\n",
    "#for 'Warengruppe' 6 the temperature is different for the same date as the other 'Warengruppe'\n",
    "discrepancies = df_inputated.groupby('Datum').agg({\n",
    "    'Bewoelkung': pd.Series.nunique,\n",
    "    'Temperatur': pd.Series.nunique,\n",
    "    'Windgeschwindigkeit': pd.Series.nunique\n",
    "}).reset_index() \n",
    "discrepancies = discrepancies[(discrepancies['Bewoelkung'] > 1) | (discrepancies['Temperatur'] > 1) | (discrepancies['Windgeschwindigkeit'] > 1)]\n",
    "print(\"Discrepancies in weather data for the same date:\")\n",
    "print(discrepancies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438c599",
   "metadata": {},
   "source": [
    "uses value of WarenGruppe 1 to all groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e83db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit']\n",
    "\n",
    "for date in discrepancies['Datum']:\n",
    "    weather_values = df_inputated.loc[df_inputated['Datum'] == date, ['Warengruppe'] + cols]\n",
    "    reference_values = weather_values.loc[weather_values['Warengruppe'] == 1, cols].iloc[0]\n",
    "    # >>> use .values para evitar alinhamento por rótulo <<<\n",
    "    df_inputated.loc[df_inputated['Datum'] == date, cols] = reference_values.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3ff98",
   "metadata": {},
   "source": [
    "For from 01.08.2018 to 31.07.2019 set id and Warengroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0fc988a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warengruppe 6 is available on the following month-day combinations:\n",
      "['10-24' '10-25' '10-26' '10-27' '10-29' '10-30' '10-31' '11-01' '11-02'\n",
      " '11-03' '11-04' '11-05' '11-06' '11-07' '11-08' '11-09' '11-10' '11-11'\n",
      " '11-12' '11-13' '11-14' '11-15' '11-16' '11-17' '11-18' '11-19' '11-20'\n",
      " '11-21' '11-22' '11-23' '11-24' '11-25' '11-26' '11-27' '11-28' '11-29'\n",
      " '11-30' '12-01' '12-02' '12-03' '12-04' '12-05' '12-06' '12-07' '12-08'\n",
      " '12-09' '12-10' '12-11' '12-12' '12-13' '12-14' '12-15' '12-16' '12-17'\n",
      " '12-18' '12-19' '12-20' '12-21' '12-22' '12-23' '12-24' '12-27' '12-29'\n",
      " '12-30' '01-02' '10-28' '12-28' '01-03' '01-04' '01-05' '01-06' '12-31']\n"
     ]
    }
   ],
   "source": [
    "#check for what month and days (no year) warengruppe 6 is avalable every year\n",
    "mask = df_inputated['Warengruppe'] == 6\n",
    "# ensure the 'Datum' values are datetime-like before using .dt (do not overwrite original column)\n",
    "date_series = pd.to_datetime(df_inputated.loc[mask, 'Datum'], errors='coerce')\n",
    "warengruppe6_dates = date_series.dt.strftime('%m-%d').dropna().unique()\n",
    "print(\"Warengruppe 6 is available on the following month-day combinations:\")\n",
    "print(warengruppe6_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93c23caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Datum</th>\n",
       "      <th>Warengruppe</th>\n",
       "      <th>Umsatz</th>\n",
       "      <th>KielerWoche</th>\n",
       "      <th>Bewoelkung</th>\n",
       "      <th>Temperatur</th>\n",
       "      <th>Windgeschwindigkeit</th>\n",
       "      <th>Wettercode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1307011.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.828353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1307012.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>535.856285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1307013.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>201.198426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1307014.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.890169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1307015.0</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>5.0</td>\n",
       "      <td>317.475875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.8375</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      Datum  Warengruppe      Umsatz  KielerWoche  Bewoelkung  \\\n",
       "0  1307011.0 2013-07-01          1.0  148.828353          0.0         6.0   \n",
       "1  1307012.0 2013-07-01          2.0  535.856285          0.0         6.0   \n",
       "2  1307013.0 2013-07-01          3.0  201.198426          0.0         6.0   \n",
       "3  1307014.0 2013-07-01          4.0   65.890169          0.0         6.0   \n",
       "4  1307015.0 2013-07-01          5.0  317.475875          0.0         6.0   \n",
       "\n",
       "   Temperatur  Windgeschwindigkeit  Wettercode  \n",
       "0     17.8375                 15.0        20.0  \n",
       "1     17.8375                 15.0        20.0  \n",
       "2     17.8375                 15.0        20.0  \n",
       "3     17.8375                 15.0        20.0  \n",
       "4     17.8375                 15.0        20.0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# máscara fornecida\n",
    "mask = (df_inputated['Datum'] >= '2018-08-01') & (df_inputated['Datum'] <= '2019-07-31')\n",
    "\n",
    "# lista das datas que devem ter 6 grupos\n",
    "\n",
    "# garante que Datum é datetime\n",
    "df_inputated[\"Datum\"] = pd.to_datetime(df_inputated[\"Datum\"])\n",
    "\n",
    "# separa o bloco que será expandido\n",
    "df_to_expand = df_inputated.loc[mask, [\"Datum\"]].drop_duplicates()\n",
    "\n",
    "def expand_groups(row):\n",
    "    date = row[\"Datum\"]\n",
    "    if date in warengruppe6_dates:\n",
    "        grupos = range(1, 7)   # 1..6\n",
    "    else:\n",
    "        grupos = range(1, 6)   # 1..5\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        \"Datum\": [date] * len(grupos),\n",
    "        \"Warengruppe\": grupos\n",
    "    })\n",
    "\n",
    "# gera linhas expandidas\n",
    "expanded_rows = pd.concat(df_to_expand.apply(expand_groups, axis=1).to_list(), ignore_index=True)\n",
    "\n",
    "# cria ID (YYMMDD + warengruppe)\n",
    "expanded_rows[\"YYMMDD\"] = expanded_rows[\"Datum\"].dt.strftime(\"%y%m%d\")\n",
    "expanded_rows[\"id\"] = expanded_rows[\"YYMMDD\"] + expanded_rows[\"Warengruppe\"].astype(str)\n",
    "expanded_rows.drop(columns=[\"YYMMDD\"], inplace=True)\n",
    "\n",
    "# remove as linhas originais que estavam dentro da máscara\n",
    "df_inputated = df_inputated.loc[~mask]\n",
    "\n",
    "# adiciona as novas linhas expandidas\n",
    "df_inputated = pd.concat([df_inputated, expanded_rows], ignore_index=True)\n",
    "\n",
    "# reordena se quiser\n",
    "df_inputated = df_inputated.sort_values(by=[\"Datum\", \"Warengruppe\"], ignore_index=True)\n",
    "\n",
    "df_inputated.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d13dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemplo da sua lista\n",
    "dates_umsatz_null = pd.to_datetime([\n",
    "    \"2013-12-25\", \"2013-12-26\", \"2014-01-01\",\n",
    "    \"2014-04-18\", \"2014-05-01\", \"2014-05-03\",\n",
    "    \"2014-05-04\", \"2014-08-17\", \"2014-12-25\",\n",
    "    \"2014-12-26\", \"2015-01-01\", \"2015-04-03\",\n",
    "    \"2015-05-01\", \"2015-12-25\", \"2015-12-26\",\n",
    "    \"2016-01-01\", \"2016-03-24\", \"2016-03-25\",\n",
    "    \"2016-07-07\", \"2016-07-18\", \"2016-08-14\",\n",
    "    \"2016-08-15\", \"2016-12-17\", \"2016-12-25\",\n",
    "    \"2016-12-26\", \"2017-01-01\", \"2017-04-14\",\n",
    "    \"2017-05-01\", \"2017-07-04\", \"2017-10-23\",\n",
    "    \"2017-10-31\", \"2017-12-25\", \"2017-12-26\",\n",
    "    \"2018-01-01\", \"2018-03-30\", \"2018-05-01\",\n",
    "    \"2018-05-21\"\n",
    "])\n",
    "\n",
    "# garantir datetime\n",
    "df_inputated[\"Datum\"] = pd.to_datetime(df_inputated[\"Datum\"])\n",
    "\n",
    "# manter apenas datas únicas\n",
    "unique_dates = df_inputated[\"Datum\"].drop_duplicates()\n",
    "\n",
    "# função de expansão\n",
    "def expand(row):\n",
    "    date = row[\"Datum\"]\n",
    "    if date in warengruppe6_dates:\n",
    "        groups = range(1, 7)   # 1..6\n",
    "    else:\n",
    "        groups = range(1, 6)   # 1..5\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Datum\": [date] * len(groups),\n",
    "        \"Warengruppe\": groups\n",
    "    })\n",
    "\n",
    "# aplica expansão\n",
    "expanded = pd.concat(\n",
    "    unique_dates.to_frame().apply(expand, axis=1).to_list(),\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# agora juntar VENDAS\n",
    "# merge com o original para obter Umsatz onde existir\n",
    "expanded = expanded.merge(\n",
    "    df_inputated[[\"Datum\", \"Umsatz\"]],\n",
    "    on=\"Datum\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# preencher vendas faltantes com 0\n",
    "expanded[\"Umsatz\"].fillna(0, inplace=True)\n",
    "\n",
    "# criar id = YYMMDD + Warengruppe\n",
    "expanded[\"id\"] = (\n",
    "    expanded[\"Datum\"].dt.strftime(\"%y%m%d\") +\n",
    "    expanded[\"Warengruppe\"].astype(str)\n",
    ")\n",
    "\n",
    "# esse agora é seu df final\n",
    "df_inputated = expanded.sort_values([\"Datum\", \"Warengruppe\"], ignore_index=True)\n",
    "\n",
    "df_inputated.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d6041",
   "metadata": {},
   "source": [
    "Save to csv// import from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6332ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save inputated dataframe to a new CSV file\n",
    "\n",
    "df_inputated.to_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/inputated_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inputated = pd.read_csv('/workspaces/bakery_prediction/0_DataPreparation/Processed/inputated_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6dfb5",
   "metadata": {},
   "source": [
    "5. Feature engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e13d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featured = df_inputated.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666e088",
   "metadata": {},
   "source": [
    "insert day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1181503e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Wochentag\n",
      "0  2013-07-01          1\n",
      "1  2013-07-02          2\n",
      "2  2013-07-03          3\n",
      "3  2013-07-04          4\n",
      "4  2013-07-05          5\n"
     ]
    }
   ],
   "source": [
    "#insert new column day of week as integer (1=Monday,...,7=Sunday)   \n",
    "df_featured['Wochentag'] = pd.to_datetime(df_featured['Datum']).dt.dayofweek + 1\n",
    "print(df_featured[['Datum', 'Wochentag']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b7879",
   "metadata": {},
   "source": [
    "insert holiday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdaa9379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Feiertag\n",
      "0  2013-07-01         0\n",
      "1  2013-07-02         0\n",
      "2  2013-07-03         0\n",
      "3  2013-07-04         0\n",
      "4  2013-07-05         0\n"
     ]
    }
   ],
   "source": [
    "#insert new column holiday from 2013-07-01 to 2018-07-31 \n",
    "holidays = [\n",
    "    '2013-10-03', '2013-12-25', '2013-12-26',\n",
    "    '2014-01-01', '2014-04-18', '2014-04-21',\n",
    "    '2014-05-01', '2014-05-29', '2014-06-09',\n",
    "    '2014-10-03', '2014-12-25', '2014-12-26',\n",
    "    '2015-01-01', '2015-04-03', '2015-04-06',\n",
    "    '2015-05-01', '2015-05-14', '2015-05-25',\n",
    "    '2015-10-03', '2015-12-25', '2015-12-26',\n",
    "    '2016-01-01', '2016-03-25', '2016-03-28',\n",
    "    '2016-05-01', '2016-05-05', '2016-05-16',\n",
    "    '2016-10-03', '2016-12-25', '2016-12-26',\n",
    "    '2017-01-01', '2017-04-14', '2017-04-17',\n",
    "    '2017-05-01', '2017-05-25', '2017-06-05',\n",
    "    '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26',\n",
    "    '2018-01-01', '2018-03-30', '2018-04-02',\n",
    "    '2018-05-01', '2018-05-10', '2018-05-21'\n",
    "]\n",
    "df_featured['Feiertag'] = df_featured['Datum'].isin(holidays)\n",
    "#convert boolean to integer (0 not holiday, 1 holiday)\n",
    "df_featured['Feiertag'] = df_featured['Feiertag'].astype(int)\n",
    "\n",
    "print(df_featured[['Datum', 'Feiertag']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7ee8c",
   "metadata": {},
   "source": [
    "set season from day and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cca8111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Monat  Jahreszeit\n",
      "0  2013-07-01      7           3\n",
      "1  2013-07-02      7           3\n",
      "2  2013-07-03      7           3\n",
      "3  2013-07-04      7           3\n",
      "4  2013-07-05      7           3\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# create column season like this: Winter from 22 December to 20 march, Spring from 21 March to 21 june, Summer from 22 June to 21 september, Autumn from 22 September to 21 december\n",
    "def get_season(date_or_month):\n",
    "    # If a date-like is passed (string/Timestamp/date), use exact day-month boundaries\n",
    "    if isinstance(date_or_month, (str, pd.Timestamp, datetime.date)):\n",
    "        d = pd.to_datetime(date_or_month)\n",
    "        m, day = d.month, d.day\n",
    "        if (m == 12 and day >= 22) or m in (1, 2) or (m == 3 and day <= 20):\n",
    "            return 1 #'Winter'\n",
    "        if (m == 3 and day >= 21) or m in (4, 5) or (m == 6 and day <= 21):\n",
    "            return 2 #'Spring'\n",
    "        if (m == 6 and day >= 22) or m in (7, 8) or (m == 9 and day <= 21):\n",
    "            return 3 #'Summer'\n",
    "        return 4 #'Autumn'\n",
    "    # If an integer month is passed, fall back to month-based mapping\n",
    "    m = int(date_or_month)\n",
    "    if m in (12, 1, 2):\n",
    "        return 1 #'Winter'\n",
    "    if m in (3, 4, 5):\n",
    "        return 2 #'Spring'\n",
    "    if m in (6, 7, 8):\n",
    "        return 3 #'Summer'\n",
    "    return 4 #'Autumn'\n",
    "\n",
    "df_featured['Monat'] = pd.to_datetime(df_featured['Datum']).dt.month\n",
    "# Derive season based on the full date (day-month boundaries are respected)\n",
    "df_featured['Jahreszeit'] = pd.to_datetime(df_featured['Datum']).apply(get_season)\n",
    "print(df_featured[['Datum', 'Monat', 'Jahreszeit']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6893db3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b7c98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert new column for school holidays (ferien)\n",
    "school_holidays = [\n",
    "    # Summer holidays\n",
    "    ('2013-07-22', '2013-08-31'),\n",
    "    ('2014-07-21', '2014-08-31'),\n",
    "    ('2015-07-20', '2015-08-31'),\n",
    "    ('2016-07-18', '2016-08-31'),\n",
    "    ('2017-07-17', '2017-08-31'),\n",
    "    ('2018-07-16', '2018-08-31'),\n",
    "    # Autumn holidays\n",
    "    ('2013-10-14', '2013-10-26'),\n",
    "    ('2014-10-13', '2014-10-25'),\n",
    "    ('2015-10-12', '2015-10-24'),\n",
    "    ('2016-10-10', '2016-10-22'),\n",
    "    ('2017-10-09', '2017-10-21'),\n",
    "    ('2018-10-08', '2018-10-20'),\n",
    "    # Christmas holidays\n",
    "    ('2013-12-23', '2014-01-05'),\n",
    "    ('2014-12-22', '2015-01-04'),\n",
    "    ('2015-12-21', '2016-01-03'),\n",
    "    ('2016-12-23', '2017-01-06'),\n",
    "    ('2017-12-22', '2018-01-05'),\n",
    "    # Winter holidays\n",
    "    ('2014-02-03', '2014-02-15'),\n",
    "    ('2015-02-02', '2015-02-14'),\n",
    "    ('2016-02-01', '2016-02-13'),\n",
    "    ('2017-01-30', '2017-02-11'),\n",
    "    ('2018-01-29', '2018-02-10'),\n",
    "    # Easter holidays\n",
    "    ('2013-03-25', '2013-04-06'),\n",
    "    ('2014-04-14', '2014-04-25'),\n",
    "    ('2015-04-06', '2015-04-18'),\n",
    "    ('2016-03-21', '2016-04-01'),\n",
    "    ('2017-04-10', '2017-04-21'),\n",
    "    ('2018-03-26', '2018-04-06'),\n",
    "    # Spring holidays\n",
    "    ('2013-05-20', '2013-05-31'),\n",
    "    ('2014-05-19', '2014-05-30'),\n",
    "    ('2015-05-18', '2015-05-29'),\n",
    "    ('2016-05-16', '2016-05-27'),\n",
    "    ('2017-05-15', '2017-05-26'),\n",
    "    ('2018-05-14', '2018-05-25'),\n",
    "]       \n",
    "df_featured['Ferien'] = 0\n",
    "for start, end in school_holidays:\n",
    "    mask = (pd.to_datetime(df_featured['Datum']) >= pd.to_datetime(start)) & (pd.to_datetime(df_featured['Datum']) <= pd.to_datetime(end))\n",
    "    df_featured.loc[mask, 'Ferien'] = 1     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19fa4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save featured dataframe to a new CSV file\n",
    "df_featured.to_csv(r'C:\\Users\\giuli\\Documents\\Open_Campus\\bakery_prediction\\0_DataPreparation\\Processed\\featured_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
   "language": "python",
   "name": "python3"
<<<<<<< Updated upstream
=======
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.1"
=======
   "version": "3.10.11"
<<<<<<< Updated upstream
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
>>>>>>> eefa1a7c6733ddff00ea56488a8479c97665df54
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
